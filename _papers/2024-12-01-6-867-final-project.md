---
title: "Low Complexity Solutions For Interpolating Deep Neural Networks"
collection: papers
category: manuscripts
permalink: /paper/2024-12-01-6-867-final-project
excerpt: 'Presents low complexity solutions for interpolating deep neural networks, including a novel algorithm and empirical results.'
date: 2024-12-10
paperurl: '/files/6.867-project.pdf'
citation: 'McManus, M., Chowdhuri, R., & Vogelbaum, E. H. (2022). &quot;Low Complexity Solutions For Interpolating Deep Neural Networks.&quot;'
abstract: |
  A generally understood fact of classical learning theory is the tradeoff between bias and variance, often drawn in textbooks as a U-shaped curve with model complexity on the x-axis and generalization error on the y-axis (Luxburg and Sch√∂lkopf, 2008). The theory posits that test error is related to the complexity of the hypothesis space from which we choose our model in a manner that has a single "sweet-spot" beyond which increasing model complexity increases test error. However, this theory has been challenged in recent years by the remarkable success of deep neural network models, which are often highly overparameterized (Canziani et al., 2016). Surprisingly, these models can generalize better as complexity increases, a phenomenon known as the "double descent curve."

  In this project, we extend the work of Belkin et al. (2019) to deep neural networks, developing a novel algorithm for learning low-complexity deep neural networks and evaluating it against baselines. Our results show that for certain classes of models, our algorithm can produce comparatively better deep neural networks, and that lower complexity deep neural networks lead to lower complexity intermediate features. These results provide further evidence that small norm interpolating models have very desirable properties which merit further exploration.

  (See PDF for full details.)
--- 