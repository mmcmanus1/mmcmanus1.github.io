# Trust-region safeguarding: KL regularisation & ratio clipping

Think it could also be beneficial to walk through the theoretical aspect of RL fine-tuning and by constraining the policy updated through either clipping the probability ratios or KL Divergence Penalty it keeps the models in check

